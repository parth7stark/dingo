################################################################################
##  Job submission arguments
################################################################################
accounting = bdwx-dtai-gh
local = False  # Run locally instead of submitting to a cluster
; generation-pool = local

# Importance sampling jobs allocation
n-parallel = 4  # Each job runs on 1 GPU (total 4 GPUs in use)
request-cpus-importance-sampling = 16  # Total 64 CPUs â†’ 16 per job

# Resource allocation for inference  #SBATCH params
request-cpus = 64  # Full node CPUs for all jobs
request-memory = 60  # Full node memory

# SLURM Scheduler Configuration
scheduler = slurm
scheduler-args = nodes=1 ntasks-per-node=4 cpus-per-task=16 mem=240g gpus-per-node=4 partition=ghx4 job-name=dingo time=00:10:00 account=bdwx-dtai-gh
scheduler-env = /u/parthpatel7173/miniconda3/etc/profile.d/conda.sh #source /u/parthpatel7173/miniconda3/etc/profile.d/conda.sh
conda-env = /u/parthpatel7173/miniconda3/envs/vfl
scheduler-analysis-time = 00:10:00

; # Importance sampling jobs allocation
; n-parallel = 4  # GPUS allocated is 4 (1 node)
; request-cpus-importance-sampling = 16   #no of cpus allocated/n-parallel

; # Resoruce allocation for inference  #SBATCH params
; request-cpus = 64
; request-memory = 240

; scheduler = slurm
; scheduler-args = mem=64g nodes=1 cpus-per-task=32 gpus-per-task=1 ntasks-per-node=2 partition=ghx4 gpus-per-node=1 job-name=dingo time=00:10:00 account=bdwx-dtai-gh
; scheduler-env = vfl
; scheduler-analysis-time=00:10:00



################################################################################
##  Sampler arguments
################################################################################

model-init = /work/hdd/bcbw/parthpatel7173/GW_Parameter_Estimation/quasicircular_precessing_spin/3000mpc_hm_HL/model_init.pt
model = /work/hdd/bcbw/parthpatel7173/GW_Parameter_Estimation/quasicircular_precessing_spin/3000mpc_hm_HL/model_init.pt
device = 'cuda'
num-gnpe-iterations = 10 # Number of Gibbs sampling iterations
num-samples = 10000   # Number of posterior samples
batch-size = 10000  # Batch size for inference
recover-log-prob = true
# prior-dict = {
# luminosity_distance = bilby.gw.prior.UniformComovingVolume(minimum=100, maximum=2000, name='luminosity_distance'),
# }
#printed self._density_recovery_settings and giving that as input (only change 100000 samples) and running this command in dingo repo
density-recovery-settings = {'num_samples': 10000, 'threshold_std': 5, 'nde_settings': {'model': {'posterior_model_type': 'normalizing_flow', 'posterior_kwargs': {'num_flow_steps': 5, 'base_transform_kwargs': {'hidden_dim': 256, 'num_transform_blocks': 4, 'activation': 'elu', 'dropout_probability': 0.1, 'batch_norm': True, 'num_bins': 8, 'base_transform_type': 'rq-coupling'}}}, 'training': {'num_workers': 0, 'train_fraction': 0.9, 'batch_size': 4096, 'epochs': 20, 'optimizer': {'type': 'adam', 'lr': 0.002}, 'scheduler': {'type': 'cosine', 'T_max': 20}}}}


################################################################################
## Data generation arguments
################################################################################

trigger-time = GW200219_094415
label = GW200219_094415 # Custom label for output files
outdir = outdir_GW200219_094415  # Directory to save results

# Detector arguments
channel-dict = {H1:GWOSC, L1:GWOSC}  # Data channels to use (H1, L1 from GWOSC)
psd-length = 128 # Length of power spectral density estimation
sampling-frequency = 4096

################################################################################
## Plotting arguments
################################################################################

plot-corner = true
plot-weights = true
plot-log-probs = true

################################################################################
## Runtime options
################################################################################

; verbose = true  # Enable detailed logs